{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Conv2D, Conv1D,MaxPooling2D, UpSampling2D, MaxPooling1D, BatchNormalization, UpSampling1D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pyf/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elapsed time</th>\n",
       "      <th>Driver</th>\n",
       "      <th>ECG</th>\n",
       "      <th>EMG</th>\n",
       "      <th>foot GSR</th>\n",
       "      <th>hand GSR</th>\n",
       "      <th>HR</th>\n",
       "      <th>marker</th>\n",
       "      <th>RESP</th>\n",
       "      <th>stress</th>\n",
       "      <th>...</th>\n",
       "      <th>foot meanGSR</th>\n",
       "      <th>hand meanGSR</th>\n",
       "      <th>foot meanSCR</th>\n",
       "      <th>hand meanSCR</th>\n",
       "      <th>foot maxSCR</th>\n",
       "      <th>hand maxSCR</th>\n",
       "      <th>foot meanSCL</th>\n",
       "      <th>hand slopeSCL</th>\n",
       "      <th>foot slopeSCL</th>\n",
       "      <th>hand meanSCL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'0:00.000'</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.002029</td>\n",
       "      <td>0.124</td>\n",
       "      <td>9.051</td>\n",
       "      <td>19.072</td>\n",
       "      <td>90.0</td>\n",
       "      <td>12.36</td>\n",
       "      <td>39.97</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>8.941716</td>\n",
       "      <td>18.961706</td>\n",
       "      <td>0.043065</td>\n",
       "      <td>0.074294</td>\n",
       "      <td>0.163574</td>\n",
       "      <td>0.349776</td>\n",
       "      <td>-0.043065</td>\n",
       "      <td>3.293266</td>\n",
       "      <td>3.554833</td>\n",
       "      <td>-0.074294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'0:00.002'</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.005351</td>\n",
       "      <td>0.124</td>\n",
       "      <td>9.051</td>\n",
       "      <td>19.072</td>\n",
       "      <td>90.0</td>\n",
       "      <td>12.36</td>\n",
       "      <td>39.97</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>8.941716</td>\n",
       "      <td>18.961706</td>\n",
       "      <td>0.043065</td>\n",
       "      <td>0.074294</td>\n",
       "      <td>0.163574</td>\n",
       "      <td>0.349776</td>\n",
       "      <td>-0.043065</td>\n",
       "      <td>3.293266</td>\n",
       "      <td>3.554833</td>\n",
       "      <td>-0.074294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'0:00.004'</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.008661</td>\n",
       "      <td>0.124</td>\n",
       "      <td>9.051</td>\n",
       "      <td>19.072</td>\n",
       "      <td>90.0</td>\n",
       "      <td>12.36</td>\n",
       "      <td>39.97</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>8.941716</td>\n",
       "      <td>18.961706</td>\n",
       "      <td>0.043065</td>\n",
       "      <td>0.074294</td>\n",
       "      <td>0.163574</td>\n",
       "      <td>0.349776</td>\n",
       "      <td>-0.043065</td>\n",
       "      <td>3.293266</td>\n",
       "      <td>3.554833</td>\n",
       "      <td>-0.074294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'0:00.006'</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.011926</td>\n",
       "      <td>0.124</td>\n",
       "      <td>9.051</td>\n",
       "      <td>19.072</td>\n",
       "      <td>90.0</td>\n",
       "      <td>12.36</td>\n",
       "      <td>39.97</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>8.941716</td>\n",
       "      <td>18.961706</td>\n",
       "      <td>0.043065</td>\n",
       "      <td>0.074294</td>\n",
       "      <td>0.163574</td>\n",
       "      <td>0.349776</td>\n",
       "      <td>-0.043065</td>\n",
       "      <td>3.293266</td>\n",
       "      <td>3.554833</td>\n",
       "      <td>-0.074294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'0:00.008'</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.015088</td>\n",
       "      <td>0.124</td>\n",
       "      <td>9.051</td>\n",
       "      <td>19.072</td>\n",
       "      <td>90.0</td>\n",
       "      <td>12.36</td>\n",
       "      <td>39.97</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>8.941716</td>\n",
       "      <td>18.961706</td>\n",
       "      <td>0.043065</td>\n",
       "      <td>0.074294</td>\n",
       "      <td>0.163574</td>\n",
       "      <td>0.349776</td>\n",
       "      <td>-0.043065</td>\n",
       "      <td>3.293266</td>\n",
       "      <td>3.554833</td>\n",
       "      <td>-0.074294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Elapsed time Driver       ECG    EMG  foot GSR  hand GSR    HR  marker  \\\n",
       "0   '0:00.000'      6 -0.002029  0.124     9.051    19.072  90.0   12.36   \n",
       "1   '0:00.002'      6 -0.005351  0.124     9.051    19.072  90.0   12.36   \n",
       "2   '0:00.004'      6 -0.008661  0.124     9.051    19.072  90.0   12.36   \n",
       "3   '0:00.006'      6 -0.011926  0.124     9.051    19.072  90.0   12.36   \n",
       "4   '0:00.008'      6 -0.015088  0.124     9.051    19.072  90.0   12.36   \n",
       "\n",
       "    RESP  stress  ...  foot meanGSR  hand meanGSR  foot meanSCR  hand meanSCR  \\\n",
       "0  39.97       1  ...      8.941716     18.961706      0.043065      0.074294   \n",
       "1  39.97       1  ...      8.941716     18.961706      0.043065      0.074294   \n",
       "2  39.97       1  ...      8.941716     18.961706      0.043065      0.074294   \n",
       "3  39.97       1  ...      8.941716     18.961706      0.043065      0.074294   \n",
       "4  39.97       1  ...      8.941716     18.961706      0.043065      0.074294   \n",
       "\n",
       "   foot maxSCR  hand maxSCR  foot meanSCL  hand slopeSCL  foot slopeSCL  \\\n",
       "0     0.163574     0.349776     -0.043065       3.293266       3.554833   \n",
       "1     0.163574     0.349776     -0.043065       3.293266       3.554833   \n",
       "2     0.163574     0.349776     -0.043065       3.293266       3.554833   \n",
       "3     0.163574     0.349776     -0.043065       3.293266       3.554833   \n",
       "4     0.163574     0.349776     -0.043065       3.293266       3.554833   \n",
       "\n",
       "   hand meanSCL  \n",
       "0     -0.074294  \n",
       "1     -0.074294  \n",
       "2     -0.074294  \n",
       "3     -0.074294  \n",
       "4     -0.074294  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('processedData.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(df, norm = False):\n",
    "    X = df.drop(['Elapsed time', 'Driver', 'marker', 'stress'], axis = 1)\n",
    "    y = df['stress']\n",
    "    if norm:\n",
    "        X=(X-X.min())/(X.max()-X.min()) # Normalising to rescale all the features. \n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y)\n",
    "    encoded_Y = encoder.transform(y)\n",
    "    trainX, testX, trainy, testy = train_test_split(X, encoded_Y, test_size = 0.2 )\n",
    "    num_features = len(X.columns)\n",
    "    return trainX, testX, trainy, testy, num_features\n",
    "\n",
    "def make_model(input_shape):\n",
    "    '''\n",
    "    This architecture is the same architecture as proposed by Azar et al.\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=input_shape, activation='relu'))\n",
    "    model.add(Dense(60, activation = 'relu'))\n",
    "    model.add(Dense(60, activation = 'relu'))\n",
    "    model.add(Dense(60, activation = 'relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam' ,metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model without Encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainy, testy, num_features = train_test(df)\n",
    "model = make_model(input_shape = num_features)\n",
    "model.fit(trainX, trainy, epochs = 125,shuffle=True ,validation_split = 0.2)\n",
    "op = model.evaluate(testX, testy)\n",
    "print(\"Test Accuracy=\", op[1]*100, '%')\n",
    "print(\"Test Loss=\", op[0])\n",
    "model.save('NN_v1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding data using Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training autoencoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Output tensors to a Model must be the output of a Keras `Layer` (thus holding past layer metadata). Found: <keras.layers.core.Dense object at 0x7fc190554f98>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-2711470b36f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mencoded_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m     93\u001b[0m             \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m                                  \u001b[0;34m'the output of a Keras `Layer` '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                                  \u001b[0;34m'(thus holding past layer metadata). '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                                  'Found: ' + str(x))\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         self._compute_previous_mask = (\n",
      "\u001b[0;31mValueError\u001b[0m: Output tensors to a Model must be the output of a Keras `Layer` (thus holding past layer metadata). Found: <keras.layers.core.Dense object at 0x7fc190554f98>"
     ]
    }
   ],
   "source": [
    "encoding_dim = 5\n",
    "input_dim = trainX.shape[1]\n",
    "\n",
    "input_vec = Input(shape=(input_dim,))\n",
    "encoded = Dense(32, activation='relu')(input_vec)\n",
    "encoded = Dense(16, activation='relu')(encoded)\n",
    "encoded = Dense(8, activation='relu')(encoded)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "\n",
    "decoded = Dense(8, activation='sigmoid')(encoded)\n",
    "decoded = Dense(16, activation='sigmoid')(decoded)\n",
    "decoded = Dense(32, activation='sigmoid')(decoded)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "\n",
    "# encoder = Model(input_vec, encoded)\n",
    "# encoded_input = Input(shape=(encoding_dim,))\n",
    "# decoder_layer = autoencoder.layers[5]\n",
    "# decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "# decoder.summary()\n",
    "\n",
    "autoencoder = Model(input_vec, decoded)\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "autoencoder.fit(trainX, trainX,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 26)                0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 32)                864       \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 5)                 45        \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 8)                 48        \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 26)                858       \n",
      "=================================================================\n",
      "Total params: 3,167\n",
      "Trainable params: 3,167\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding and decoding training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38436, 26)\n",
      "(9609, 26)\n"
     ]
    }
   ],
   "source": [
    "trainX_anc = autoencoder.predict(trainX) # Encoding and decoding the training data\n",
    "testX_anc = autoencoder.predict(testX) # Encoding and decoding the testing data\n",
    "print(trainX_anc.shape)\n",
    "print(testX_anc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training NN model on encoded-decoded data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_123 (Dense)            (None, 60)                1620      \n",
      "_________________________________________________________________\n",
      "dense_124 (Dense)            (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_126 (Dense)            (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_127 (Dense)            (None, 1)                 61        \n",
      "=================================================================\n",
      "Total params: 12,661\n",
      "Trainable params: 12,661\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 30748 samples, validate on 7688 samples\n",
      "Epoch 1/125\n",
      "30748/30748 [==============================] - 4s 115us/step - loss: 0.5676 - accuracy: 0.6930 - val_loss: 0.5559 - val_accuracy: 0.6822\n",
      "Epoch 2/125\n",
      "30748/30748 [==============================] - 3s 105us/step - loss: 0.5616 - accuracy: 0.6968 - val_loss: 0.5556 - val_accuracy: 0.7060\n",
      "Epoch 3/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.5605 - accuracy: 0.6980 - val_loss: 0.5535 - val_accuracy: 0.7042\n",
      "Epoch 4/125\n",
      "30748/30748 [==============================] - 3s 104us/step - loss: 0.5592 - accuracy: 0.6999 - val_loss: 0.5535 - val_accuracy: 0.7036\n",
      "Epoch 5/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.5574 - accuracy: 0.7051 - val_loss: 0.5544 - val_accuracy: 0.7060\n",
      "Epoch 6/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.5528 - accuracy: 0.7110 - val_loss: 0.5431 - val_accuracy: 0.7270\n",
      "Epoch 7/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.5432 - accuracy: 0.7219 - val_loss: 0.5309 - val_accuracy: 0.7356\n",
      "Epoch 8/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.5313 - accuracy: 0.7286 - val_loss: 0.5225 - val_accuracy: 0.7294\n",
      "Epoch 9/125\n",
      "30748/30748 [==============================] - 3s 103us/step - loss: 0.5243 - accuracy: 0.7339 - val_loss: 0.5185 - val_accuracy: 0.7373\n",
      "Epoch 10/125\n",
      "30748/30748 [==============================] - 3s 103us/step - loss: 0.5247 - accuracy: 0.7313 - val_loss: 0.5173 - val_accuracy: 0.7386\n",
      "Epoch 11/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.5206 - accuracy: 0.7342 - val_loss: 0.5126 - val_accuracy: 0.7408\n",
      "Epoch 12/125\n",
      "30748/30748 [==============================] - 3s 100us/step - loss: 0.5189 - accuracy: 0.7362 - val_loss: 0.5098 - val_accuracy: 0.7409\n",
      "Epoch 13/125\n",
      "30748/30748 [==============================] - 3s 103us/step - loss: 0.5160 - accuracy: 0.7381 - val_loss: 0.5151 - val_accuracy: 0.7391\n",
      "Epoch 14/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.5142 - accuracy: 0.7399 - val_loss: 0.5155 - val_accuracy: 0.7445\n",
      "Epoch 15/125\n",
      "30748/30748 [==============================] - 3s 103us/step - loss: 0.5161 - accuracy: 0.7429 - val_loss: 0.5332 - val_accuracy: 0.7306\n",
      "Epoch 16/125\n",
      "30748/30748 [==============================] - 3s 103us/step - loss: 0.5156 - accuracy: 0.7440 - val_loss: 0.5108 - val_accuracy: 0.7448\n",
      "Epoch 17/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.5144 - accuracy: 0.7433 - val_loss: 0.5062 - val_accuracy: 0.7566\n",
      "Epoch 18/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.5137 - accuracy: 0.7448 - val_loss: 0.5093 - val_accuracy: 0.7434\n",
      "Epoch 19/125\n",
      "30748/30748 [==============================] - 3s 101us/step - loss: 0.5132 - accuracy: 0.7444 - val_loss: 0.5145 - val_accuracy: 0.7514\n",
      "Epoch 20/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.5089 - accuracy: 0.7461 - val_loss: 0.5017 - val_accuracy: 0.7548\n",
      "Epoch 21/125\n",
      "30748/30748 [==============================] - 3s 101us/step - loss: 0.5075 - accuracy: 0.7463 - val_loss: 0.5021 - val_accuracy: 0.7521\n",
      "Epoch 22/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.5057 - accuracy: 0.7486 - val_loss: 0.5082 - val_accuracy: 0.7464\n",
      "Epoch 23/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.5056 - accuracy: 0.7464 - val_loss: 0.5095 - val_accuracy: 0.7439\n",
      "Epoch 24/125\n",
      "30748/30748 [==============================] - 3s 96us/step - loss: 0.5045 - accuracy: 0.7478 - val_loss: 0.4995 - val_accuracy: 0.7551\n",
      "Epoch 25/125\n",
      "30748/30748 [==============================] - 3s 104us/step - loss: 0.5050 - accuracy: 0.7485 - val_loss: 0.4994 - val_accuracy: 0.7540\n",
      "Epoch 26/125\n",
      "30748/30748 [==============================] - 3s 103us/step - loss: 0.5046 - accuracy: 0.7482 - val_loss: 0.5048 - val_accuracy: 0.7548\n",
      "Epoch 27/125\n",
      "30748/30748 [==============================] - 3s 105us/step - loss: 0.5057 - accuracy: 0.7470 - val_loss: 0.5098 - val_accuracy: 0.7488\n",
      "Epoch 28/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.5035 - accuracy: 0.7479 - val_loss: 0.4950 - val_accuracy: 0.7559\n",
      "Epoch 29/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.5026 - accuracy: 0.7501 - val_loss: 0.4949 - val_accuracy: 0.7539\n",
      "Epoch 30/125\n",
      "30748/30748 [==============================] - 3s 101us/step - loss: 0.5023 - accuracy: 0.7492 - val_loss: 0.5124 - val_accuracy: 0.7401\n",
      "Epoch 31/125\n",
      "30748/30748 [==============================] - 3s 104us/step - loss: 0.5021 - accuracy: 0.7479 - val_loss: 0.4955 - val_accuracy: 0.7521\n",
      "Epoch 32/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.5011 - accuracy: 0.7488 - val_loss: 0.4933 - val_accuracy: 0.7547\n",
      "Epoch 33/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.5020 - accuracy: 0.7491 - val_loss: 0.5053 - val_accuracy: 0.7513\n",
      "Epoch 34/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.5017 - accuracy: 0.7504 - val_loss: 0.5061 - val_accuracy: 0.7496\n",
      "Epoch 35/125\n",
      "30748/30748 [==============================] - 3s 103us/step - loss: 0.5030 - accuracy: 0.7489 - val_loss: 0.5082 - val_accuracy: 0.7531\n",
      "Epoch 36/125\n",
      "30748/30748 [==============================] - 3s 101us/step - loss: 0.5030 - accuracy: 0.7494 - val_loss: 0.4969 - val_accuracy: 0.7535\n",
      "Epoch 37/125\n",
      "30748/30748 [==============================] - 3s 103us/step - loss: 0.5028 - accuracy: 0.7482 - val_loss: 0.5108 - val_accuracy: 0.7523\n",
      "Epoch 38/125\n",
      "30748/30748 [==============================] - 3s 101us/step - loss: 0.5019 - accuracy: 0.7501 - val_loss: 0.5003 - val_accuracy: 0.7535\n",
      "Epoch 39/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.5013 - accuracy: 0.7488 - val_loss: 0.5025 - val_accuracy: 0.7590\n",
      "Epoch 40/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.5011 - accuracy: 0.7496 - val_loss: 0.4944 - val_accuracy: 0.7492\n",
      "Epoch 41/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.5003 - accuracy: 0.7495 - val_loss: 0.5022 - val_accuracy: 0.7471\n",
      "Epoch 42/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.4998 - accuracy: 0.7498 - val_loss: 0.4925 - val_accuracy: 0.7569\n",
      "Epoch 43/125\n",
      "30748/30748 [==============================] - 3s 103us/step - loss: 0.4992 - accuracy: 0.7504 - val_loss: 0.5020 - val_accuracy: 0.7566\n",
      "Epoch 44/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.5006 - accuracy: 0.7503 - val_loss: 0.5245 - val_accuracy: 0.7382\n",
      "Epoch 45/125\n",
      "30748/30748 [==============================] - 3s 103us/step - loss: 0.4993 - accuracy: 0.7499 - val_loss: 0.4944 - val_accuracy: 0.7577\n",
      "Epoch 46/125\n",
      "30748/30748 [==============================] - 3s 106us/step - loss: 0.4992 - accuracy: 0.7503 - val_loss: 0.4921 - val_accuracy: 0.7557\n",
      "Epoch 47/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.4986 - accuracy: 0.7498 - val_loss: 0.4969 - val_accuracy: 0.7534\n",
      "Epoch 48/125\n",
      "30748/30748 [==============================] - 3s 100us/step - loss: 0.4993 - accuracy: 0.7506 - val_loss: 0.4926 - val_accuracy: 0.7534\n",
      "Epoch 49/125\n",
      "30748/30748 [==============================] - 3s 99us/step - loss: 0.4995 - accuracy: 0.7500 - val_loss: 0.5050 - val_accuracy: 0.7518\n",
      "Epoch 50/125\n",
      "30748/30748 [==============================] - 3s 99us/step - loss: 0.5006 - accuracy: 0.7492 - val_loss: 0.4982 - val_accuracy: 0.7540\n",
      "Epoch 51/125\n",
      "30748/30748 [==============================] - 3s 100us/step - loss: 0.4982 - accuracy: 0.7511 - val_loss: 0.4915 - val_accuracy: 0.7575\n",
      "Epoch 52/125\n",
      "30748/30748 [==============================] - 3s 100us/step - loss: 0.5004 - accuracy: 0.7481 - val_loss: 0.5020 - val_accuracy: 0.7553\n",
      "Epoch 53/125\n",
      "30748/30748 [==============================] - 3s 98us/step - loss: 0.4995 - accuracy: 0.7498 - val_loss: 0.4927 - val_accuracy: 0.7560\n",
      "Epoch 54/125\n",
      "30748/30748 [==============================] - 3s 98us/step - loss: 0.5018 - accuracy: 0.7524 - val_loss: 0.5010 - val_accuracy: 0.7557\n",
      "Epoch 55/125\n",
      "30748/30748 [==============================] - 3s 101us/step - loss: 0.5053 - accuracy: 0.7510 - val_loss: 0.5051 - val_accuracy: 0.7514\n",
      "Epoch 56/125\n",
      "30748/30748 [==============================] - 3s 100us/step - loss: 0.5045 - accuracy: 0.7522 - val_loss: 0.5010 - val_accuracy: 0.7518\n",
      "Epoch 57/125\n",
      "30748/30748 [==============================] - 3s 100us/step - loss: 0.5032 - accuracy: 0.7525 - val_loss: 0.5074 - val_accuracy: 0.7508\n",
      "Epoch 58/125\n",
      "30748/30748 [==============================] - 3s 100us/step - loss: 0.5029 - accuracy: 0.7511 - val_loss: 0.5026 - val_accuracy: 0.7510\n",
      "Epoch 59/125\n",
      "30748/30748 [==============================] - 3s 99us/step - loss: 0.5032 - accuracy: 0.7503 - val_loss: 0.4978 - val_accuracy: 0.7551\n",
      "Epoch 60/125\n",
      "30748/30748 [==============================] - 3s 100us/step - loss: 0.5020 - accuracy: 0.7507 - val_loss: 0.5028 - val_accuracy: 0.7514\n",
      "Epoch 61/125\n",
      "30748/30748 [==============================] - 3s 100us/step - loss: 0.5008 - accuracy: 0.7507 - val_loss: 0.4980 - val_accuracy: 0.7556\n",
      "Epoch 62/125\n",
      "30748/30748 [==============================] - 3s 100us/step - loss: 0.4993 - accuracy: 0.7526 - val_loss: 0.5277 - val_accuracy: 0.7360\n",
      "Epoch 63/125\n",
      "30748/30748 [==============================] - 3s 99us/step - loss: 0.4993 - accuracy: 0.7512 - val_loss: 0.4954 - val_accuracy: 0.7555\n",
      "Epoch 64/125\n",
      "30748/30748 [==============================] - 3s 99us/step - loss: 0.4991 - accuracy: 0.7514 - val_loss: 0.4961 - val_accuracy: 0.7586\n",
      "Epoch 65/125\n",
      "30748/30748 [==============================] - 3s 100us/step - loss: 0.4982 - accuracy: 0.7509 - val_loss: 0.4945 - val_accuracy: 0.7552\n",
      "Epoch 66/125\n",
      "30748/30748 [==============================] - 3s 99us/step - loss: 0.4993 - accuracy: 0.7515 - val_loss: 0.4940 - val_accuracy: 0.7639\n",
      "Epoch 67/125\n",
      "30748/30748 [==============================] - 3s 104us/step - loss: 0.4978 - accuracy: 0.7516 - val_loss: 0.4929 - val_accuracy: 0.7590\n",
      "Epoch 68/125\n",
      "30748/30748 [==============================] - 3s 103us/step - loss: 0.4972 - accuracy: 0.7521 - val_loss: 0.4923 - val_accuracy: 0.7547\n",
      "Epoch 69/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.4959 - accuracy: 0.7529 - val_loss: 0.4957 - val_accuracy: 0.7553\n",
      "Epoch 70/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.4962 - accuracy: 0.7524 - val_loss: 0.4945 - val_accuracy: 0.7536\n",
      "Epoch 71/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.4977 - accuracy: 0.7516 - val_loss: 0.4903 - val_accuracy: 0.7621\n",
      "Epoch 72/125\n",
      "30748/30748 [==============================] - 3s 101us/step - loss: 0.4963 - accuracy: 0.7527 - val_loss: 0.5025 - val_accuracy: 0.7551\n",
      "Epoch 73/125\n",
      "30748/30748 [==============================] - 3s 101us/step - loss: 0.4978 - accuracy: 0.7522 - val_loss: 0.4937 - val_accuracy: 0.7613\n",
      "Epoch 74/125\n",
      "30748/30748 [==============================] - 3s 106us/step - loss: 0.4964 - accuracy: 0.7515 - val_loss: 0.4904 - val_accuracy: 0.7566\n",
      "Epoch 75/125\n",
      "30748/30748 [==============================] - 3s 101us/step - loss: 0.4952 - accuracy: 0.7534 - val_loss: 0.4944 - val_accuracy: 0.7560\n",
      "Epoch 76/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.4948 - accuracy: 0.7528 - val_loss: 0.4913 - val_accuracy: 0.7613\n",
      "Epoch 77/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.4941 - accuracy: 0.7518 - val_loss: 0.4952 - val_accuracy: 0.7618\n",
      "Epoch 78/125\n",
      "30748/30748 [==============================] - 3s 103us/step - loss: 0.4943 - accuracy: 0.7528 - val_loss: 0.5000 - val_accuracy: 0.7617\n",
      "Epoch 79/125\n",
      "30748/30748 [==============================] - 3s 101us/step - loss: 0.4954 - accuracy: 0.7516 - val_loss: 0.5007 - val_accuracy: 0.7399\n",
      "Epoch 80/125\n",
      "30748/30748 [==============================] - 3s 104us/step - loss: 0.4956 - accuracy: 0.7522 - val_loss: 0.4998 - val_accuracy: 0.7487\n",
      "Epoch 81/125\n",
      "30748/30748 [==============================] - 3s 103us/step - loss: 0.4963 - accuracy: 0.7506 - val_loss: 0.5068 - val_accuracy: 0.7520\n",
      "Epoch 82/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.4955 - accuracy: 0.7529 - val_loss: 0.4916 - val_accuracy: 0.7560\n",
      "Epoch 83/125\n",
      "30748/30748 [==============================] - 3s 103us/step - loss: 0.4959 - accuracy: 0.7527 - val_loss: 0.4928 - val_accuracy: 0.7542\n",
      "Epoch 84/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.4961 - accuracy: 0.7515 - val_loss: 0.5060 - val_accuracy: 0.7535\n",
      "Epoch 85/125\n",
      "30748/30748 [==============================] - 3s 101us/step - loss: 0.4957 - accuracy: 0.7519 - val_loss: 0.4881 - val_accuracy: 0.7631\n",
      "Epoch 86/125\n",
      "30748/30748 [==============================] - 3s 101us/step - loss: 0.4954 - accuracy: 0.7526 - val_loss: 0.4918 - val_accuracy: 0.7624\n",
      "Epoch 87/125\n",
      "30748/30748 [==============================] - 3s 103us/step - loss: 0.4940 - accuracy: 0.7535 - val_loss: 0.5026 - val_accuracy: 0.7522\n",
      "Epoch 88/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.4945 - accuracy: 0.7540 - val_loss: 0.4957 - val_accuracy: 0.7557\n",
      "Epoch 89/125\n",
      "30748/30748 [==============================] - 3s 103us/step - loss: 0.4944 - accuracy: 0.7527 - val_loss: 0.4895 - val_accuracy: 0.7608\n",
      "Epoch 90/125\n",
      "30748/30748 [==============================] - 3s 103us/step - loss: 0.4927 - accuracy: 0.7548 - val_loss: 0.4924 - val_accuracy: 0.7552\n",
      "Epoch 91/125\n",
      "30748/30748 [==============================] - 3s 103us/step - loss: 0.4942 - accuracy: 0.7536 - val_loss: 0.4866 - val_accuracy: 0.7591\n",
      "Epoch 92/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.4940 - accuracy: 0.7523 - val_loss: 0.4893 - val_accuracy: 0.7616\n",
      "Epoch 93/125\n",
      "30748/30748 [==============================] - 3s 109us/step - loss: 0.4922 - accuracy: 0.7546 - val_loss: 0.4905 - val_accuracy: 0.7574\n",
      "Epoch 94/125\n",
      "30748/30748 [==============================] - 3s 107us/step - loss: 0.4930 - accuracy: 0.7536 - val_loss: 0.4924 - val_accuracy: 0.7596\n",
      "Epoch 95/125\n",
      "30748/30748 [==============================] - 3s 106us/step - loss: 0.4915 - accuracy: 0.7552 - val_loss: 0.4870 - val_accuracy: 0.7621\n",
      "Epoch 96/125\n",
      "30748/30748 [==============================] - 3s 107us/step - loss: 0.4913 - accuracy: 0.7535 - val_loss: 0.4901 - val_accuracy: 0.7553\n",
      "Epoch 97/125\n",
      "30748/30748 [==============================] - 3s 108us/step - loss: 0.4919 - accuracy: 0.7518 - val_loss: 0.4850 - val_accuracy: 0.7575\n",
      "Epoch 98/125\n",
      "30748/30748 [==============================] - 3s 111us/step - loss: 0.4911 - accuracy: 0.7533 - val_loss: 0.4886 - val_accuracy: 0.7581\n",
      "Epoch 99/125\n",
      "30748/30748 [==============================] - 4s 122us/step - loss: 0.4921 - accuracy: 0.7532 - val_loss: 0.4848 - val_accuracy: 0.7661\n",
      "Epoch 100/125\n",
      "30748/30748 [==============================] - 3s 107us/step - loss: 0.4927 - accuracy: 0.7531 - val_loss: 0.4893 - val_accuracy: 0.7543\n",
      "Epoch 101/125\n",
      "30748/30748 [==============================] - 3s 112us/step - loss: 0.4911 - accuracy: 0.7538 - val_loss: 0.5024 - val_accuracy: 0.7414\n",
      "Epoch 102/125\n",
      "30748/30748 [==============================] - 3s 108us/step - loss: 0.4916 - accuracy: 0.7547 - val_loss: 0.4944 - val_accuracy: 0.7535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/125\n",
      "30748/30748 [==============================] - 3s 104us/step - loss: 0.4898 - accuracy: 0.7539 - val_loss: 0.4860 - val_accuracy: 0.7605\n",
      "Epoch 104/125\n",
      "30748/30748 [==============================] - 3s 103us/step - loss: 0.4904 - accuracy: 0.7538 - val_loss: 0.4896 - val_accuracy: 0.7516\n",
      "Epoch 105/125\n",
      "30748/30748 [==============================] - 3s 100us/step - loss: 0.4891 - accuracy: 0.7551 - val_loss: 0.4864 - val_accuracy: 0.7653\n",
      "Epoch 106/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.4905 - accuracy: 0.7546 - val_loss: 0.4913 - val_accuracy: 0.7581\n",
      "Epoch 107/125\n",
      "30748/30748 [==============================] - 3s 100us/step - loss: 0.4911 - accuracy: 0.7542 - val_loss: 0.4872 - val_accuracy: 0.7572\n",
      "Epoch 108/125\n",
      "30748/30748 [==============================] - 3s 101us/step - loss: 0.4894 - accuracy: 0.7534 - val_loss: 0.5024 - val_accuracy: 0.7530\n",
      "Epoch 109/125\n",
      "30748/30748 [==============================] - 3s 100us/step - loss: 0.4899 - accuracy: 0.7538 - val_loss: 0.4868 - val_accuracy: 0.7560\n",
      "Epoch 110/125\n",
      "30748/30748 [==============================] - 3s 100us/step - loss: 0.4900 - accuracy: 0.7545 - val_loss: 0.4847 - val_accuracy: 0.7612\n",
      "Epoch 111/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.4896 - accuracy: 0.7543 - val_loss: 0.4900 - val_accuracy: 0.7618\n",
      "Epoch 112/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.4911 - accuracy: 0.7566 - val_loss: 0.4836 - val_accuracy: 0.7613\n",
      "Epoch 113/125\n",
      "30748/30748 [==============================] - 3s 109us/step - loss: 0.4918 - accuracy: 0.7532 - val_loss: 0.5036 - val_accuracy: 0.7552\n",
      "Epoch 114/125\n",
      "30748/30748 [==============================] - 3s 108us/step - loss: 0.4910 - accuracy: 0.7530 - val_loss: 0.4913 - val_accuracy: 0.7523\n",
      "Epoch 115/125\n",
      "30748/30748 [==============================] - 3s 108us/step - loss: 0.4906 - accuracy: 0.7554 - val_loss: 0.4854 - val_accuracy: 0.7608\n",
      "Epoch 116/125\n",
      "30748/30748 [==============================] - 4s 120us/step - loss: 0.4899 - accuracy: 0.7552 - val_loss: 0.4845 - val_accuracy: 0.7600\n",
      "Epoch 117/125\n",
      "30748/30748 [==============================] - 3s 101us/step - loss: 0.4889 - accuracy: 0.7532 - val_loss: 0.4878 - val_accuracy: 0.7604\n",
      "Epoch 118/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.4907 - accuracy: 0.7535 - val_loss: 0.4838 - val_accuracy: 0.7583\n",
      "Epoch 119/125\n",
      "30748/30748 [==============================] - 3s 102us/step - loss: 0.4887 - accuracy: 0.7544 - val_loss: 0.4862 - val_accuracy: 0.7581\n",
      "Epoch 120/125\n",
      "30748/30748 [==============================] - 6s 204us/step - loss: 0.4895 - accuracy: 0.7559 - val_loss: 0.4817 - val_accuracy: 0.7605\n",
      "Epoch 121/125\n",
      "30748/30748 [==============================] - 3s 105us/step - loss: 0.4892 - accuracy: 0.7565 - val_loss: 0.5009 - val_accuracy: 0.7510\n",
      "Epoch 122/125\n",
      "30748/30748 [==============================] - 3s 105us/step - loss: 0.4898 - accuracy: 0.7546 - val_loss: 0.4993 - val_accuracy: 0.7551\n",
      "Epoch 123/125\n",
      "30748/30748 [==============================] - 4s 115us/step - loss: 0.4894 - accuracy: 0.7542 - val_loss: 0.4847 - val_accuracy: 0.7590\n",
      "Epoch 124/125\n",
      "30748/30748 [==============================] - 4s 138us/step - loss: 0.4892 - accuracy: 0.7545 - val_loss: 0.4874 - val_accuracy: 0.7579\n",
      "Epoch 125/125\n",
      "30748/30748 [==============================] - 4s 114us/step - loss: 0.4914 - accuracy: 0.7536 - val_loss: 0.4866 - val_accuracy: 0.7608\n",
      "9609/9609 [==============================] - 1s 150us/step\n",
      "Test Accuracy after encoding-decoding = 75.67905187606812 %\n",
      "Test Loss= 0.4889734837270475\n"
     ]
    }
   ],
   "source": [
    "postEnc_model = make_model(input_shape = trainX_anc.shape[1])\n",
    "postEnc_model.fit(trainX_anc, trainy, epochs = 125,shuffle=True ,validation_split = 0.2)\n",
    "op = postEnc_model.evaluate(testX_anc, testy)\n",
    "print(\"Test Accuracy after encoding-decoding =\", op[1]*100, '%')\n",
    "print(\"Test Loss=\", op[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy went down to 75% (from 95% acc without encoding data) after encoding-decoding the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_35 (InputLayer)        (None, 26)                0         \n",
      "_________________________________________________________________\n",
      "dense_208 (Dense)            (None, 16)                432       \n",
      "_________________________________________________________________\n",
      "dense_209 (Dense)            (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_210 (Dense)            (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_211 (Dense)            (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_212 (Dense)            (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_213 (Dense)            (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_214 (Dense)            (None, 5)                 45        \n",
      "_________________________________________________________________\n",
      "dense_215 (Dense)            (None, 8)                 48        \n",
      "_________________________________________________________________\n",
      "dense_216 (Dense)            (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_217 (Dense)            (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_218 (Dense)            (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_219 (Dense)            (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_220 (Dense)            (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_221 (Dense)            (None, 26)                442       \n",
      "=================================================================\n",
      "Total params: 2,623\n",
      "Trainable params: 2,623\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "38436/38436 [==============================] - 2s 55us/step - loss: 0.6068\n",
      "Epoch 2/100\n",
      "38436/38436 [==============================] - 1s 23us/step - loss: 0.5552\n",
      "Epoch 3/100\n",
      "38436/38436 [==============================] - 1s 24us/step - loss: 0.5516\n",
      "Epoch 4/100\n",
      "38436/38436 [==============================] - 1s 24us/step - loss: 0.5514\n",
      "Epoch 5/100\n",
      "38436/38436 [==============================] - 1s 27us/step - loss: 0.5514\n",
      "Epoch 6/100\n",
      "38436/38436 [==============================] - 1s 24us/step - loss: 0.5504\n",
      "Epoch 7/100\n",
      "38436/38436 [==============================] - 1s 23us/step - loss: 0.5254\n",
      "Epoch 8/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4886\n",
      "Epoch 9/100\n",
      "38436/38436 [==============================] - 1s 23us/step - loss: 0.4805\n",
      "Epoch 10/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4784\n",
      "Epoch 11/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4777\n",
      "Epoch 12/100\n",
      "38436/38436 [==============================] - 1s 23us/step - loss: 0.4774\n",
      "Epoch 13/100\n",
      "38436/38436 [==============================] - 1s 23us/step - loss: 0.4774\n",
      "Epoch 14/100\n",
      "38436/38436 [==============================] - 1s 23us/step - loss: 0.4773\n",
      "Epoch 15/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4773\n",
      "Epoch 16/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4773\n",
      "Epoch 17/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4773\n",
      "Epoch 18/100\n",
      "38436/38436 [==============================] - 1s 23us/step - loss: 0.4773\n",
      "Epoch 19/100\n",
      "38436/38436 [==============================] - 1s 23us/step - loss: 0.4773\n",
      "Epoch 20/100\n",
      "38436/38436 [==============================] - 1s 23us/step - loss: 0.4773\n",
      "Epoch 21/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4773\n",
      "Epoch 22/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4773\n",
      "Epoch 23/100\n",
      "38436/38436 [==============================] - 1s 23us/step - loss: 0.4773\n",
      "Epoch 24/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4773\n",
      "Epoch 25/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4773\n",
      "Epoch 26/100\n",
      "38436/38436 [==============================] - 1s 23us/step - loss: 0.4773\n",
      "Epoch 27/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4773\n",
      "Epoch 28/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4773\n",
      "Epoch 29/100\n",
      "38436/38436 [==============================] - 1s 23us/step - loss: 0.4773\n",
      "Epoch 30/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4773\n",
      "Epoch 31/100\n",
      "38436/38436 [==============================] - 1s 23us/step - loss: 0.4773\n",
      "Epoch 32/100\n",
      "38436/38436 [==============================] - 1s 23us/step - loss: 0.4773\n",
      "Epoch 33/100\n",
      "38436/38436 [==============================] - 1s 23us/step - loss: 0.4773\n",
      "Epoch 34/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4773\n",
      "Epoch 35/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4773\n",
      "Epoch 36/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4773\n",
      "Epoch 37/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4773\n",
      "Epoch 38/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4773\n",
      "Epoch 39/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4774\n",
      "Epoch 40/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4774\n",
      "Epoch 41/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4773\n",
      "Epoch 42/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4773\n",
      "Epoch 43/100\n",
      "38436/38436 [==============================] - 1s 23us/step - loss: 0.4773\n",
      "Epoch 44/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4710\n",
      "Epoch 45/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4584\n",
      "Epoch 46/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4580\n",
      "Epoch 47/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 48/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4579\n",
      "Epoch 49/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 50/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4579\n",
      "Epoch 51/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 52/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4579\n",
      "Epoch 53/100\n",
      "38436/38436 [==============================] - 1s 23us/step - loss: 0.4579\n",
      "Epoch 54/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 55/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 56/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 57/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4579\n",
      "Epoch 58/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 59/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 60/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4579\n",
      "Epoch 61/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4579\n",
      "Epoch 62/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 63/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4579\n",
      "Epoch 64/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4579\n",
      "Epoch 65/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 66/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 67/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 68/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4579\n",
      "Epoch 69/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4579\n",
      "Epoch 70/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 71/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 72/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 73/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 74/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 75/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 76/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4579\n",
      "Epoch 77/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4579\n",
      "Epoch 78/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 79/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 80/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4579\n",
      "Epoch 81/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 82/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4579\n",
      "Epoch 83/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 84/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4579\n",
      "Epoch 85/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 86/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4579\n",
      "Epoch 87/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4579\n",
      "Epoch 88/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 89/100\n",
      "38436/38436 [==============================] - 1s 21us/step - loss: 0.4579\n",
      "Epoch 90/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4579\n",
      "Epoch 91/100\n",
      "38436/38436 [==============================] - 1s 22us/step - loss: 0.4579\n",
      "Epoch 92/100\n",
      "38436/38436 [==============================] - 1s 23us/step - loss: 0.4579\n",
      "Epoch 93/100\n",
      "38436/38436 [==============================] - 1s 29us/step - loss: 0.4579\n",
      "Epoch 94/100\n",
      "38436/38436 [==============================] - 3s 66us/step - loss: 0.4579\n",
      "Epoch 95/100\n",
      "38436/38436 [==============================] - 1s 28us/step - loss: 0.4579\n",
      "Epoch 96/100\n",
      "38436/38436 [==============================] - 1s 23us/step - loss: 0.4579\n",
      "Epoch 97/100\n",
      "38436/38436 [==============================] - 1s 23us/step - loss: 0.4579\n",
      "Epoch 98/100\n",
      "38436/38436 [==============================] - 1s 23us/step - loss: 0.4579\n",
      "Epoch 99/100\n",
      "38436/38436 [==============================] - 3s 67us/step - loss: 0.4579\n",
      "Epoch 100/100\n",
      "38436/38436 [==============================] - 1s 24us/step - loss: 0.4579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fc12fc06cc0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_dim = 5\n",
    "input_dim = trainX.shape[1]\n",
    "\n",
    "input_vec = Input(shape=(input_dim,))\n",
    "encoded = Dense(16, activation='relu')(input_vec)\n",
    "encoded = Dense(16, activation='relu')(encoded)\n",
    "encoded = Dense(16, activation='relu')(encoded)\n",
    "encoded = Dense(8, activation='relu')(encoded)\n",
    "encoded = Dense(8, activation='relu')(encoded)\n",
    "encoded = Dense(8, activation='relu')(encoded)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(8, activation='sigmoid')(encoded)\n",
    "decoded = Dense(8, activation='sigmoid')(decoded)\n",
    "decoded = Dense(8, activation='sigmoid')(decoded)\n",
    "decoded = Dense(16, activation='sigmoid')(decoded)\n",
    "decoded = Dense(16, activation='sigmoid')(decoded)\n",
    "decoded = Dense(16, activation='sigmoid')(decoded)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder2 = Model(input_vec, decoded)\n",
    "autoencoder2.summary()\n",
    "\n",
    "autoencoder2.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder2.fit(trainX, trainX,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38436, 26)\n",
      "(9609, 26)\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_189 (Dense)            (None, 60)                1620      \n",
      "_________________________________________________________________\n",
      "dense_190 (Dense)            (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_191 (Dense)            (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_192 (Dense)            (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_193 (Dense)            (None, 1)                 61        \n",
      "=================================================================\n",
      "Total params: 12,661\n",
      "Trainable params: 12,661\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 30748 samples, validate on 7688 samples\n",
      "Epoch 1/100\n",
      "30748/30748 [==============================] - 5s 146us/step - loss: 0.6144 - accuracy: 0.6042 - val_loss: 0.6066 - val_accuracy: 0.6094\n",
      "Epoch 2/100\n",
      "30748/30748 [==============================] - 4s 127us/step - loss: 0.6126 - accuracy: 0.6061 - val_loss: 0.6074 - val_accuracy: 0.6094\n",
      "Epoch 3/100\n",
      "30748/30748 [==============================] - 4s 127us/step - loss: 0.6123 - accuracy: 0.6061 - val_loss: 0.6076 - val_accuracy: 0.6094\n",
      "Epoch 4/100\n",
      "30748/30748 [==============================] - 4s 127us/step - loss: 0.6125 - accuracy: 0.6061 - val_loss: 0.6066 - val_accuracy: 0.6094\n",
      "Epoch 5/100\n",
      "30748/30748 [==============================] - 4s 128us/step - loss: 0.6122 - accuracy: 0.6061 - val_loss: 0.6075 - val_accuracy: 0.6094\n",
      "Epoch 6/100\n",
      "30748/30748 [==============================] - 4s 126us/step - loss: 0.6122 - accuracy: 0.6061 - val_loss: 0.6068 - val_accuracy: 0.6094\n",
      "Epoch 7/100\n",
      "30748/30748 [==============================] - 4s 126us/step - loss: 0.6124 - accuracy: 0.6061 - val_loss: 0.6074 - val_accuracy: 0.6094\n",
      "Epoch 8/100\n",
      "30748/30748 [==============================] - 4s 126us/step - loss: 0.6120 - accuracy: 0.6061 - val_loss: 0.6075 - val_accuracy: 0.6094\n",
      "Epoch 9/100\n",
      "30748/30748 [==============================] - 4s 126us/step - loss: 0.6121 - accuracy: 0.6061 - val_loss: 0.6069 - val_accuracy: 0.6094\n",
      "Epoch 10/100\n",
      "30748/30748 [==============================] - 4s 126us/step - loss: 0.6121 - accuracy: 0.6061 - val_loss: 0.6070 - val_accuracy: 0.6094\n",
      "Epoch 11/100\n",
      "30432/30748 [============================>.] - ETA: 0s - loss: 0.6118 - accuracy: 0.6063"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-6f9a69fcdc0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel_enc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainX_anc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel_enc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX_anc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_enc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX_anc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Accuracy after encoding-decoding =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    208\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                                          \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                                          verbose=0)\n\u001b[0m\u001b[1;32m    211\u001b[0m                     \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                     \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3251\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3253\u001b[0;31m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3254\u001b[0m     \u001b[0mfeed_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3255\u001b[0m     \u001b[0marray_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m(op_input_list)\u001b[0m\n\u001b[1;32m    460\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    870\u001b[0m   \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m   \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m       \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/_weakrefset.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_IterationGuard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mitemref\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitemref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainX_anc = autoencoder2.predict(trainX) # Encoding and decoding the training data\n",
    "testX_anc = autoencoder2.predict(testX) # Encoding and decoding the testing data\n",
    "print(trainX_anc.shape)\n",
    "print(testX_anc.shape)\n",
    "\n",
    "model_enc2 = make_model(input_shape = trainX_anc.shape[1]) \n",
    "model_enc2.fit(trainX_anc, trainy, epochs = 100,shuffle=True ,validation_split = 0.2)\n",
    "op = model_enc2.evaluate(testX_anc, testy)\n",
    "print(\"Test Accuracy after encoding-decoding =\", op[1]*100, '%')\n",
    "print(\"Test Loss=\", op[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders v3\n",
    "Here, we will focus on compressing the depth (i.e. number of records) in the dataset rather than compressing the breath (i.e. dimensions or number of features). \n",
    "This is based on this (https://www.sciencedirect.com/science/article/pii/S1389041718302730) approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17b    4960\n",
       "16     4960\n",
       "11     4960\n",
       "10     4960\n",
       "9      4960\n",
       "8      4960\n",
       "7      4960\n",
       "6      4960\n",
       "17a    4800\n",
       "12     3008\n",
       "12      557\n",
       "Name: Driver, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfv3 = df.copy(deep = True)\n",
    "drivers = list(dfv3['Driver'].value_counts().keys())\n",
    "dfv3['Driver'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_55 (InputLayer)        (None, 2000, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_207 (Conv1D)          (None, 2000, 8)           32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_73 (MaxPooling (None, 1000, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_208 (Conv1D)          (None, 1000, 32)          1312      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 1000, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_74 (MaxPooling (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_209 (Conv1D)          (None, 500, 16)           1552      \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 500, 16)           64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_75 (MaxPooling (None, 250, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_210 (Conv1D)          (None, 250, 64)           11328     \n",
      "_________________________________________________________________\n",
      "conv1d_211 (Conv1D)          (None, 250, 128)          106624    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_76 (MaxPooling (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_212 (Conv1D)          (None, 125, 32)           12320     \n",
      "_________________________________________________________________\n",
      "conv1d_213 (Conv1D)          (None, 125, 1)            225       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_77 (MaxPooling (None, 63, 1)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_214 (Conv1D)          (None, 63, 1)             8         \n",
      "_________________________________________________________________\n",
      "conv1d_215 (Conv1D)          (None, 63, 32)            128       \n",
      "_________________________________________________________________\n",
      "up_sampling1d_29 (UpSampling (None, 126, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_216 (Conv1D)          (None, 116, 64)           22592     \n",
      "_________________________________________________________________\n",
      "conv1d_217 (Conv1D)          (None, 104, 128)          106624    \n",
      "_________________________________________________________________\n",
      "up_sampling1d_30 (UpSampling (None, 208, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_218 (Conv1D)          (None, 206, 16)           6160      \n",
      "_________________________________________________________________\n",
      "conv1d_219 (Conv1D)          (None, 202, 32)           2592      \n",
      "_________________________________________________________________\n",
      "up_sampling1d_31 (UpSampling (None, 404, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_220 (Conv1D)          (None, 402, 32)           3104      \n",
      "_________________________________________________________________\n",
      "up_sampling1d_32 (UpSampling (None, 804, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_221 (Conv1D)          (None, 802, 8)            776       \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 6416)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2000)              12834000  \n",
      "=================================================================\n",
      "Total params: 13,109,569\n",
      "Trainable params: 13,109,473\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2724\n"
     ]
    }
   ],
   "source": [
    "def CAE(time_steps = 2000):\n",
    "    \n",
    "    input_sample = Input(shape=(time_steps, 1))\n",
    "    encoding = Conv1D(filters=8, kernel_size=3, activation='relu', padding = 'same')(input_sample)\n",
    "    encoding = MaxPooling1D(pool_size = 2, padding = 'same')(encoding)\n",
    "    encoding = Conv1D(filters = 32, kernel_size = 5, activation = 'relu', padding = 'same')(encoding)\n",
    "    encoding = BatchNormalization()(encoding)\n",
    "    encoding = MaxPooling1D(pool_size = 2, padding = 'same')(encoding)\n",
    "    \n",
    "    encoding = Conv1D(filters = 16, kernel_size = 3, activation = 'relu', padding = 'same')(encoding)\n",
    "    encoding = BatchNormalization()(encoding)\n",
    "    encoding = MaxPooling1D(pool_size = 2, padding = 'same')(encoding)\n",
    "  \n",
    "    encoding = Conv1D(filters = 64, kernel_size = 11, activation = 'relu', padding = 'same')(encoding)\n",
    "    encoding = Conv1D(filters = 128, kernel_size = 13, activation = 'relu', padding = 'same')(encoding)\n",
    "    encoding = MaxPooling1D(pool_size = 2, padding = 'same')(encoding)\n",
    "    encoding = Conv1D(filters = 32, kernel_size = 3, activation = 'relu', padding = 'same')(encoding)\n",
    "    encoding = Conv1D(filters = 1, kernel_size = 7, activation = 'relu', padding = 'same')(encoding)\n",
    "    encoding = MaxPooling1D(pool_size = 2, padding = 'same')(encoding)\n",
    "    \n",
    "    # Making decoder:\n",
    "    decoding = Conv1D(filters = 1, kernel_size = 7, padding='same', activation = 'relu')(encoding)\n",
    "    decoding = Conv1D(32, 3, activation = 'relu', padding = 'same')(decoding)\n",
    "    decoding = UpSampling1D()(decoding)\n",
    "    decoding = Conv1D(64, 11, activation = 'relu')(decoding)\n",
    "    decoding = Conv1D(128, 13, activation = 'relu')(decoding)\n",
    "    decoding = UpSampling1D()(decoding)\n",
    "    decoding = Conv1D(16, 3, activation = 'relu')(decoding)\n",
    "    decoding = Conv1D(32, 5, activation = 'relu')(decoding)\n",
    "    decoding = UpSampling1D()(decoding)\n",
    "    decoding = Conv1D(32, 3, activation = 'relu')(decoding)\n",
    "    decoding = UpSampling1D()(decoding)\n",
    "    decoding = Conv1D(8, 3, activation = 'relu')(decoding)\n",
    "    decoding = Flatten()(decoding)\n",
    "    decoding = Dense(time_steps, activation = 'sigmoid')(decoding)\n",
    "    \n",
    "    model = Model(input_sample, decoding)\n",
    "    model.compile(optimizer='adadelta', loss='mean_squared_error')\n",
    "    \n",
    "    # Making a seperate encoder \n",
    "    encoder = Model(input_sample, encoding)\n",
    "    \n",
    "    return model, encoder\n",
    "\n",
    "mdl, encoder = CAE()\n",
    "mdl.summary()\n",
    "\n",
    "x = np.array(dfv3.loc[:1999, ['ECG']])\n",
    "x = x.reshape(-1, 2000, 1)\n",
    "x.shape\n",
    "mdl.fit(x, x.reshape(-1, 2000))\n",
    "\n",
    "temp = encoder.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 63, 1)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
